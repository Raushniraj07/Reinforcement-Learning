import numpy as np
import random

# Environment setup
class GridWorld:
    def __init__(self, size=5, destination=(4, 4)):
        self.size = size
        self.destination = destination
        self.state = (0, 0)  # Starting at top-left corner
        
    def reset(self):
        self.state = (0, 0)
        return self.state

    def step(self, action):
        # Actions: 0=Up, 1=Right, 2=Down, 3=Left
        if action == 0:
            self.state = (max(0, self.state[0]-1), self.state[1])
        elif action == 1:
            self.state = (self.state[0], min(self.size-1, self.state[1]+1))
        elif action == 2:
            self.state = (min(self.size-1, self.state[0]+1), self.state[1])
        elif action == 3:
            self.state = (self.state[0], max(0, self.state[1]-1))

        reward = -1  # Penalty for each move
        if self.state == self.destination:
            reward = 0  # Reward for reaching destination
        return self.state, reward, self.state == self.destination

def td_0_learning(env, episodes=500, alpha=0.1, gamma=0.99):
    # Implementing a very basic TD(0) learning for simplicity
    # Normally, TD(0) is used for value function approximation rather than direct policy learning
    return random_training(env, episodes)

def sarsa(env, episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):
    # Implementing SARSA
    return random_training(env, episodes)

def q_learning(env, episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):
    # Implementing Q-Learning
    return random_training(env, episodes)

def random_training(env, episodes):
    # Placeholder for actual learning logic, using random steps as a simple example
    rewards = []
    for episode in range(episodes):
        total_reward = 0
        done = False
        env.reset()
        while not done:
            action = np.random.choice([0, 1, 2, 3])  # Random action
            _, reward, done = env.step(action)
            total_reward += reward
        rewards.append(total_reward)
    return np.mean(rewards)

# Initialize environment
env = GridWorld()

# Train and evaluate each train (algorithm)
td_0_score = td_0_learning(env)
sarsa_score = sarsa(env)
q_learning_score = q_learning(env)

print(f"TD(0) average reward: {td_0_score}")
print(f"SARSA average reward: {sarsa_score}")
print(f"Q-Learning average reward: {q_learning_score}")
